{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Installation and Imports**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install transformers --quiet\n","!pip install huggingface_hub --quiet\n","!pip install -U accelerate --quiet\n","\n","!pip install -U huggingface-hub --quiet\n","!pip install datasets==2.13 --quiet\n","# !pip install nlpaug"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, RobertaForSequenceClassification, Trainer, TrainingArguments\n","from transformers import DataCollatorWithPadding\n","from IPython.display import FileLink, FileLinks\n","from datasets import Dataset, load_dataset, concatenate_datasets\n","import sklearn\n","from sklearn.metrics import accuracy_score\n","import os\n","import numpy as np\n","import torch\n","import math\n","# from torch.utils.data import DataLoader\n","# import nlpaug.augmenter.word as naw"]},{"cell_type":"markdown","metadata":{},"source":["## **Notebook Login**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token(<your_token>)\"\n","\n","from huggingface_hub import notebook_login\n","notebook_login()"]},{"cell_type":"markdown","metadata":{},"source":["# **Dataset Loading**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = load_dataset(\"Yunij/tokenized_datasets\", split=\"train\")\n","test_dataset = load_dataset(\"Yunij/tokenized_datasets\", split=\"test\")\n","\n","df_train = Dataset.to_pandas(train_dataset)\n","df_test = Dataset.to_pandas(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train.rename(columns={'label': 'human/ai'}, inplace=True)\n","df_test.rename(columns={'label': 'human/ai'}, inplace=True)\n","\n","df_train = df_train.drop(['input_ids', 'attention_mask'], axis=1)\n","df_test = df_test.drop(['input_ids', 'attention_mask'], axis=1)\n","\n","all_sources = df_train['source'].unique().tolist()\n","num_labels = len(all_sources)\n","id2label = {key: value for key, value in enumerate(all_sources)}\n","label2id = {value: key for key, value in id2label.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train['source'] = df_train['source'].map(label2id)\n","df_test['source'] = df_test['source'].map(label2id)"]},{"cell_type":"markdown","metadata":{},"source":["# **Text Cleaning**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import re\n","import nltk\n","import string\n","import subprocess\n","\n","nltk.download('stopwords')\n","# nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","# For downloading wordnet in kaggle because normal method doesn't work for wordnet in kaggle\n","# Download and unzip wordnet\n","try:\n","    nltk.data.find('wordnet.zip')\n","except:\n","    nltk.download('wordnet', download_dir='/kaggle/working/')\n","    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n","    subprocess.run(command.split())\n","    nltk.data.path.append('/kaggle/working/')\n","\n","# Now you can import the NLTK resources as usual\n","from nltk.corpus import wordnet\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["punctuations = string.punctuation\n","lemmatizer = WordNetLemmatizer()\n","\n","stopword = stopwords.words('english')\n","new_stop = [re.sub('[^a-z]', '', word) for word in stopword] #doesn't --> doesnt, can't --> cant\n","stopword.extend(new_stop)\n","stopword = list(set(stopword)) #removing duplicates"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def remove_urls(text):\n","    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n","    return re.sub(pattern, '', text)\n","\n","def remove_html(text):\n","    html_pattern = re.compile('<.*?>')\n","    return re.sub(html_pattern, '', text)\n","\n","def text_cleaning(text):\n","    text = text.replace(\"\\n\", ' ') #removing next line\n","    text = remove_urls(text) #removing urls\n","    text = remove_html(text) #removing html tags\n","    text = re.sub(r\"-\", \" \", text) #nearest-neighbor --> nearest neighbor, finite-size --> finite size\n","    text = re.sub(r\"\\$[^$]*\\$\", \"\", text) #removing formulas written as $F = ma$\n","    text = re.sub('[^a-zA-Z ]', '', text.lower()) #removing all except alphabets and spaces and changing each letter into lower case\n","    text = re.sub('( . )', ' ', text) #removing a single character word\n","    # text = text.translate(str.maketrans('', '', punctuations)) #removing punctuations\n","    text = \" \".join([lemmatizer.lemmatize(word) for word in str(text).split() if word not in stopword]) #removing stopwords and lemmatizing\n","    text = text.strip() #removing trailing spaces\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train['cleaned_text'] = df_train['text'].apply(lambda x: text_cleaning(x))\n","df_test['cleaned_text'] = df_test['text'].apply(lambda x: text_cleaning(x))\n","\n","train_dataset = Dataset.from_pandas(df_train)\n","test_dataset = Dataset.from_pandas(df_test)"]},{"cell_type":"markdown","metadata":{},"source":["# **Fine Tuning RoBERTa**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained('roberta-base',\n","                                                         num_labels=num_labels, \n","                                                         id2label=id2label, \n","                                                         label2id=label2id)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def tokenize_text(examples):\n","    return tokenizer(examples[\"cleaned_text\"], truncation=True, max_length=512)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = train_dataset.map(tokenize_text, batched=True)\n","test_dataset = test_dataset.map(tokenize_text, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset.push_to_hub(\"hc3-wiki-cleaned-text-for-domain-classification-roberta-tokenized-max-len-512\", split=\"train\")\n","test_dataset.push_to_hub(\"hc3-wiki-cleaned-text-for-domain-classification-roberta-tokenized-max-len-512\", split=\"test\")"]},{"cell_type":"markdown","metadata":{},"source":["# **Checkpoint for training** "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = load_dataset(\"rajendrabaskota/hc3-wiki-cleaned-text-for-domain-classification-roberta-tokenized-max-len-512\", split=\"train\")\n","test_dataset = load_dataset(\"rajendrabaskota/hc3-wiki-cleaned-text-for-domain-classification-roberta-tokenized-max-len-512\", split=\"test\")\n","\n","df_train = Dataset.to_pandas(train_dataset)\n","df_test = Dataset.to_pandas(test_dataset)\n","\n","all_sources = df_train['source'].unique().tolist()\n","num_labels = len(all_sources)\n","id2label = {key: value for key, value in enumerate(all_sources)}\n","label2id = {value: key for key, value in id2label.items()}\n","\n","train_dataset = train_dataset.rename_column(\"source\", \"label\")\n","test_dataset = test_dataset.rename_column(\"source\", \"label\")\n","\n","train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n","test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained('roberta-base', \n","                                                         num_labels=num_labels, \n","                                                         id2label=id2label, \n","                                                         label2id=label2id)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class_weights = (1 - (df_train[\"source\"].value_counts().sort_index() / (len(df_train)+len(df_test)))).values\n","class_weights = torch.from_numpy(class_weights).float().to(\"cuda\")\n","\n","class WeightedLossTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\")\n","        # forward pass\n","        outputs = model(**inputs)\n","        logits = outputs.get(\"logits\")\n","        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n","        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","        return (loss, outputs) if return_outputs else loss"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["batch_size = 16\n","epochs = 1\n","output_dir = \"hc3-wiki-domain-classification-roberta\"\n","logging_steps = len(train_dataset) // batch_size\n","training_args = TrainingArguments(output_dir,\n","                                  num_train_epochs=epochs,\n","                                  per_device_train_batch_size=batch_size,\n","                                  per_device_eval_batch_size=batch_size,\n","                                  evaluation_strategy=\"steps\",\n","                                  eval_steps=400,\n","                                  logging_strategy=\"steps\",\n","                                  logging_steps=400,\n","                                  learning_rate=5e-5,\n","                                  weight_decay=0.01,\n","#                                   save_strategy=\"no\"\n","                                  save_steps=400,\n","#                                   load_best_model_at_end=True,\n","                                  save_total_limit=2,\n","#                                   push_to_hub=False\n","                                 )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_accuracy_scores = []\n","test_f1_scores = []\n","train_accuracy_scores = []\n","train_f1_scores = []\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    print(f\"labels: {labels.shape}\")\n","    print(f\"preds: {preds.shape}\")\n","    accuracy = accuracy_score(labels, preds)\n","    f1 = sklearn.metrics.f1_score(labels, preds, average='micro')\n","    test_accuracy_scores.append(accuracy)\n","    test_f1_scores.append(f1)\n","    \n","    return {'accuracy': accuracy, 'f1_score': f1}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer = WeightedLossTrainer(model=model, \n","                              args=training_args,\n","                              compute_metrics=compute_metrics,\n","                              train_dataset=train_dataset,\n","                              eval_dataset=test_dataset,\n","                              tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["FileLinks(f\"hc3-wiki-domain-classification-roberta/checkpoint-10200\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.push_to_hub(\"hc3-wiki-domain-classification-roberta-1-epoch\")"]},{"cell_type":"markdown","metadata":{},"source":["# **Result and Inference**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_ckpt = \"rajendrabaskota/hc3-wiki-domain-classification-roberta\"\n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","\n","trainer = Trainer(model=model, \n","                  tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = load_dataset(\"rajendrabaskota/hc3-wiki-cleaned-text-for-domain-classification-roberta-tokenized-max-len-512\", split=\"train\")\n","test_dataset = load_dataset(\"rajendrabaskota/hc3-wiki-cleaned-text-for-domain-classification-roberta-tokenized-max-len-512\", split=\"test\")\n","\n","df_train = Dataset.to_pandas(train_dataset)\n","df_test = Dataset.to_pandas(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mean_perplexities = df_train[['source', 'perplexity']].groupby('source').mean().to_dict()\n","mean_perplexities = mean_perplexities['perplexity']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mean_perplexities"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sum = 0\n","for key, value in mean_perplexities.items():\n","    sum += value\n","    \n","print(sum/6.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["id2label = model.config.id2label\n","mean = {}\n","for key, value in mean_perplexities.items():\n","    label = id2label[key]\n","    mean[label] = value\n","    \n","print(mean)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def calculate_metrics(df, dataset):\n","    predictions, labels, _ = trainer.predict(dataset)\n","    prediction_source = np.argmax(predictions, axis=-1)\n","    \n","    df['predicted_source'] = prediction_source\n","    df['predicted_label'] = df.apply(lambda row: 1 if row['perplexity'] <= mean_perplexities[row['predicted_source']] else 0, axis=1)\n","    \n","    accuracy = accuracy_score(df['human/ai'], df['predicted_label'])\n","    f1_score = sklearn.metrics.f1_score(df['human/ai'], df['predicted_label'], average='binary')\n","    \n","    return accuracy, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_accuracy, train_f1 = calculate_metrics(df_train, train_dataset)\n","print(f\"Train Accuracy: {train_accuracy}, Train F1: {train_f1}\")\n","test_accuracy, test_f1 = calculate_metrics(df_test, test_dataset)\n","print(f\"Test Accuracy: {test_accuracy}, Test F1: {test_f1}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def compute_metrics_source_wise(df):\n","    accuracies = []\n","    f1_scores = []\n","    for i in range(6):\n","        df_temp = df[df['source'].isin([i])]\n","        accuracy = accuracy_score(df_temp['human/ai'], df_temp['predicted_label'])\n","        f1_score = sklearn.metrics.f1_score(df_temp['human/ai'], df_temp['predicted_label'], average='binary')\n","        \n","        accuracies.append(accuracy)\n","        f1_scores.append(f1_score)\n","        \n","    return accuracies, f1_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_accuracies, train_f1_scores = compute_metrics_source_wise(df_train)\n","test_accuracies, test_f1_scores = compute_metrics_source_wise(df_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_accuracies, train_f1_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_accuracies, test_f1_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["final_train_acc = {}\n","final_train_f1 = {}\n","final_test_acc = {}\n","final_test_f1 = {}\n","\n","for i, source in enumerate(all_sources):\n","    final_train_acc[source] = train_accuracies[i]\n","    final_test_acc[source] = test_accuracies[i]\n","    final_train_f1[source] = train_f1_scores[i]\n","    final_test_f1[source] = test_f1_scores[i]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f\"Train Accuracies: {final_train_acc}\")\n","print(f\"Train F1: {final_train_f1}\")\n","print(f\"Test Accuracies: {final_test_acc}\")\n","print(f\"Test F1: {final_test_f1}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.12 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
